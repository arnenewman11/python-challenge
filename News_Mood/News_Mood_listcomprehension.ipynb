{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Mood Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. On aggregate, the New York Times had the most negative tweets and CBS the most positive over the period analyzed. \n",
    "2. All of the outlets had a high level of variation in the compound scores. \n",
    "3. There was no clear trend in the aggregate polarity of tweets over time in the period analyzed. Plotting this data over an extended period of time could be very interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "#from datetime import datetime (not needed right now)\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter API Keys\n",
    "consumer_key = \"Ed4RNulN1lp7AbOooHa9STCoU\"\n",
    "consumer_secret = \"P7cUJlmJZq0VaCY0Jg7COliwQqzK0qYEyUF9Y0idx4ujb3ZlW5\"\n",
    "access_token = \"839621358724198402-dzdOsx2WWHrSuBwyNUiqSEnTivHozAZ\"\n",
    "access_token_secret = \"dCZ80uNRbFDjxdU2EckmNiSckdoATach6Q8zb7YYYE5ER\"\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8aef1e42d961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mnegative_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mneutral_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0msource_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mtweets_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtweet_dates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user' is not defined"
     ]
    }
   ],
   "source": [
    "#loop through search terms\n",
    "target_users = [\"BBC\", \"CBS\", \"CNN\", \"Fox\", \"NYT\"]\n",
    "\n",
    "dictionaries = [{},{},{},{},{}]\n",
    "\n",
    "for x in range(5):\n",
    "    \n",
    "    # Lists to hold sentiments\n",
    "    compound_list = []\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    neutral_list = []\n",
    "    source_list = []\n",
    "    tweets_text = []\n",
    "    tweet_dates = []\n",
    "    tweets_ago = []\n",
    "    tweet_counter = 0\n",
    "\n",
    "    # Grab 100 tweets\n",
    "    public_tweets = api.user_timeline(target_users[x], count=100, result_type=\"recent\", lang = \"en\")\n",
    "\n",
    "   # Loop through all tweets\n",
    "    for tweet in public_tweets:\n",
    "\n",
    "        # Run Vader Analysis on each tweet\n",
    "        compound = analyzer.polarity_scores(tweet[\"text\"])[\"compound\"]\n",
    "        pos = analyzer.polarity_scores(tweet[\"text\"])[\"pos\"]\n",
    "        neu = analyzer.polarity_scores(tweet[\"text\"])[\"neu\"]\n",
    "        neg = analyzer.polarity_scores(tweet[\"text\"])[\"neg\"]\n",
    "        tweet_text = tweet[\"text\"]\n",
    "        tweet_date = tweet[\"created_at\"]\n",
    "        \n",
    "        # Add each value to the appropriate array\n",
    "        compound_list.append(compound)\n",
    "        positive_list.append(pos)\n",
    "        negative_list.append(neg)\n",
    "        neutral_list.append(neu)\n",
    "        source_list.append(user)\n",
    "        tweets_text.append(tweet_text)\n",
    "        tweet_dates.append(tweet_date)\n",
    "        tweets_ago.append(tweet_counter)\n",
    "        tweet_counter += 1\n",
    "\n",
    "    # Store the Average Sentiments\n",
    "    dictionaries[x] = {\"Compound\": compound_list,\n",
    "                 \"Positive\": positive_list,\n",
    "                 \"Neutral\": negative_list,\n",
    "                 \"Negative\": neutral_list,\n",
    "                 \"Source\":source_list,\n",
    "                 \"Tweet Text\": tweets_text,\n",
    "                 \"Tweet Date\": tweet_dates,\n",
    "                 \"Tweets Ago\": tweets_ago}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull analysis of each source into a separate data frame\n",
    "bbc_dict = dictionaries[0]\n",
    "bbc_df = pd.DataFrame(bbc_dict)\n",
    "\n",
    "cbs_dict = dictionaries[1]\n",
    "cbs_df = pd.DataFrame(cbs_dict)\n",
    "\n",
    "cnn_dict = dictionaries[2]\n",
    "cnn_df = pd.DataFrame(cnn_dict)\n",
    "\n",
    "fox_dict = dictionaries[3]\n",
    "fox_df = pd.DataFrame(fox_dict)\n",
    "\n",
    "nyt_dict = dictionaries[4]\n",
    "nyt_df = pd.DataFrame(nyt_dict)\n",
    "\n",
    "compiled_df = [bbc_df, cbs_df, cnn_df, fox_df, nyt_df]\n",
    "\n",
    "#save a csv of all data retrieved\n",
    "data_to_store = pd.DataFrame(compiled_df)\n",
    "data_to_store.to_csv(\"news_mood_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the scatterplot\n",
    "colors = [\"coral\", \"firebrick\", \"green\", \"gold\", \"dodgerblue\"]\n",
    "color_counter = 0\n",
    "labels = [\"BBC\", \"CBS\", \"CNN\", \"Fox\", \"NYT\"]\n",
    "label_counter = 0\n",
    "\n",
    "for df in compiled_df:   \n",
    "        \n",
    "    # Build the scatter plots for each source\n",
    "    plt.scatter(df[\"Tweets Ago\"], \n",
    "                df[\"Compound\"], \n",
    "                s = 25,\n",
    "                c=colors[color_counter], \n",
    "                edgecolor=\"black\", linewidths=1, marker=\"o\", \n",
    "                alpha=0.8, label= labels[label_counter])\n",
    "\n",
    "    color_counter += 1\n",
    "    label_counter += 1\n",
    "\n",
    "\n",
    "# Incorporate the other graph properties\n",
    "plt.title(\"Sentiment Analysis of Media Tweets (12/15/17)\")\n",
    "plt.ylabel(\"Tweet Polarity\")\n",
    "plt.xlabel(\"Tweets Ago\")\n",
    "plt.xlim((0,100))\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.grid(True)\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "# Create a legend\n",
    "lgnd = plt.legend(mode=\"Expanded\", \n",
    "                  numpoints=1, scatterpoints=1, \n",
    "                  loc=\"upper right\", title=\"Media Sources\",\n",
    "                  bbox_to_anchor=(1.3, 1),\n",
    "                  labelspacing=0.5)\n",
    "lgnd.legendHandles[0]._sizes = [30]\n",
    "lgnd.legendHandles[1]._sizes = [30]\n",
    "lgnd.legendHandles[2]._sizes = [30]\n",
    "lgnd.legendHandles[3]._sizes = [30]\n",
    "lgnd.legendHandles[4]._sizes = [30]\n",
    "\n",
    "plt.savefig(\"sentiment_scatter.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df[\"Compound\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compiled_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cba30a46d4e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompiled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Compound\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0magg_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compiled_df' is not defined"
     ]
    }
   ],
   "source": [
    "#create bar chart data\n",
    "agg_scores = []\n",
    "\n",
    "for x in range(5):\n",
    "    score = compiled_df[x][\"Compound\"].mean()\n",
    "    agg_scores.append(score)\n",
    "\n",
    "#clear the plt\n",
    "plt.gcf().clear()\n",
    "\n",
    "#create the bar chart\n",
    "x_axis = np.arange(len(target_users))\n",
    "\n",
    "# Tell matplotlib that we will be making a bar chart\n",
    "# Users is our y axis and x_axis is, of course, our x axis\n",
    "# We apply align=\"edge\" to ensure our bars line up with our tick marks\n",
    "plt.bar(x_axis, agg_scores, color= colors, alpha=0.5, align=\"edge\")\n",
    "\n",
    "tick_locations = [value+0.4 for value in x_axis]\n",
    "plt.xticks(tick_locations, target_users)\n",
    "\n",
    "plt.title(\"Overall Media Sentiment based on Twitter\")\n",
    "plt.xlabel(\"Media Outlet\")\n",
    "plt.ylabel(\"Tweet Polarity\")\n",
    "\n",
    "# Sets the y limits of the current chart\n",
    "plt.ylim(-0.2, 0.5)\n",
    "\n",
    "sns.set()\n",
    "plt.savefig(\"sentiment_bar.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
